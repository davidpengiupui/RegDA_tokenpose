Namespace(source_root='data/SURREAL', target_root='data/LSP', source='SURREAL', target='LSP', resize_scale=(0.6, 1.3), rotation=180, image_size=256, heatmap_size=64, arch='resnet101', pretrain=None, resume=None, num_head_layers=2, margin=4.0, trade_off=1.0, batch_size=32, lr=0.01, momentum=0.9, wd=0.0001, lr_gamma=0.0001, lr_decay=0.75, lr_step=[45, 60], lr_factor=0.1, workers=4, pretrain_epochs=70, epochs=60, iters_per_epoch=500, print_freq=100, seed=0, log='logs/regda/surreal2lsp', phase='train', debug=True)
/data/david/RegDA/regda_lr0.01.py:44: UserWarning: You have chosen to seed training. This will turn on the CUDNN deterministic setting, which can slow down your training considerably! You may see unexpected behavior when restarting from checkpoints.
  warnings.warn('You have chosen to seed training. '
loading data/SURREAL/train/run0.json
loading data/SURREAL/train/run1.json
loading data/SURREAL/train/run2.json
loading data/SURREAL/test/run0.json
loading data/SURREAL/test/run1.json
loading data/SURREAL/test/run2.json
Source train: 14107
Target train: 62
Source test: 100
Target test: 63
/home/lab1119/anaconda3/envs/SFDA/lib/python3.9/site-packages/torchvision/transforms/functional.py:423: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/home/lab1119/anaconda3/envs/SFDA/lib/python3.9/site-packages/torchvision/transforms/functional.py:423: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/home/lab1119/anaconda3/envs/SFDA/lib/python3.9/site-packages/torchvision/transforms/functional.py:423: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/home/lab1119/anaconda3/envs/SFDA/lib/python3.9/site-packages/torchvision/transforms/functional.py:423: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
Pretraining the model on source domain.
/home/lab1119/anaconda3/envs/SFDA/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/home/lab1119/anaconda3/envs/SFDA/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
[0.001, 0.01, 0.01]
Epoch: [0][  0/500]	Time 1.93 (1.93)	Data 0.0 (0.0)	Loss (s) 4.10e+00 (4.10e+00)	Acc (s) 0.03 (0.03)
Epoch: [0][100/500]	Time 0.21 (0.23)	Data 0.0 (0.0)	Loss (s) 2.40e+00 (2.88e+00)	Acc (s) 0.19 (0.11)
Traceback (most recent call last):
  File "/data/david/RegDA/regda_lr0.01.py", line 495, in <module>
    main(args)
  File "/data/david/RegDA/regda_lr0.01.py", line 131, in main
    pretrain(train_source_iter, pretrained_model, criterion, optimizer, epoch, args)
  File "/data/david/RegDA/regda_lr0.01.py", line 260, in pretrain
    loss_s.backward()
  File "/home/lab1119/anaconda3/envs/SFDA/lib/python3.9/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/lab1119/anaconda3/envs/SFDA/lib/python3.9/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
